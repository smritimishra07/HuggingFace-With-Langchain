{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain_huggingface) (0.24.5)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain_huggingface) (0.2.30)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain_huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain_huggingface) (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.99)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smrit\\langchain\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is machine learning?\\n\\nMachine learning is a new field of research, but it is gaining mainstream acceptance and becoming a popular practice in the academic world. Machine learning is the information processing and interpretation of data that can be used to understand and plan actions. In this regard this is especially important for news articles, news stories and other information that is being produced by machine learning.\\n\\nWhat has been the most successful use of machine learning to improve journalism?\\n\\nThe main use of machine learning is to improve reporting by journalists. Machine learning has been used in newsrooms to give better reporting tools, and to modify news to imitate the coverage of other news outlets.\\n\\nWho has been the most successful machine learning researcher?\\n\\nThe most successful researcher of machine learning'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer 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\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"What is machine learning?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "model_id = \"gpt2\"  # A well-known, general-purpose model\n",
    "response = requests.post(f\"https://api-inference.huggingface.co/models/{model_id}\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"What is generative AI?\\n\\nIn the early days of AI, the term 'AI' was used to refer to any technology that could solve a problem, such as a problem of solving an algorithm. However, in the early days of AI, the term 'AI'\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API request headers and payload\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 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\",  # Replace with your API token\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"What is generative AI?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,  # Use a smaller number for testing\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "model_id = \"gpt2\"  # A well-known, general-purpose model\n",
    "response = requests.post(f\"https://api-inference.huggingface.co/models/{model_id}\", headers=headers, json=payload)\n",
    "\n",
    "# Print the raw JSON response for debugging\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"What is generative AI?\\n\\nIn the early days of AI, the term 'AI' was used to refer to any technology that could solve a problem, such as a problem of solving an algorithm. However, in the early days of AI, the term 'AI'\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API request headers and payload\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 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\",  # Replace with your API token\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"What is generative AI?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,  # Use a smaller number for testing\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "model_id = \"gpt2\"  # A well-known, general-purpose model\n",
    "response = requests.post(f\"https://api-inference.huggingface.co/models/{model_id}\", headers=headers, json=payload)\n",
    "\n",
    "# Print the raw JSON response for debugging\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
